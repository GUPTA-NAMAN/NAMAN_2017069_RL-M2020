{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Q6_Policy_Iteration.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM6EQtxybaGnIAmxTmKc+/W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GUPTA-NAMAN/Reinforcement-Learning/blob/A2/Q6_Policy_Iteration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiSL1lF8TclA"
      },
      "source": [
        "import numpy\n",
        "import random\n",
        "import copy"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQBveLVstodX"
      },
      "source": [
        "# Defining data structure for state-value function(\"value\") index (i,j)[\"which represent state\"] represent state value of BLOCK at Co-Ordinate (i,j)\n",
        "# Defining data structure for state-action function(\"action\") at index (i,j)[\"which represent state\"] we take action action(i,j)(\"which is denoted by integer for more INFO Look First-most(\"find_next_block_CoOrdinate\") function\")\n",
        "value = numpy.zeros( (4,4) , \"float\" )           # STATE-VALUE\n",
        "action = numpy.zeros( (4,4) ,\"int\" )             # POLICY\n",
        "gamma = 0.9                                      # Dis-counting factor"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W99s0HNeukHJ"
      },
      "source": [
        "# Randomly initializing state-action\n",
        "for i in range(4) :\n",
        "  for j in range(4) :\n",
        "    action[i][j] = random.uniform(1,4) \n",
        "action[0][0] = -1   # NO defined action because it is terminating state\n",
        "action[3][3] = -1"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mK6-P_EJhUYN"
      },
      "source": [
        "# It takes current position and action as input and output co_Ordunate after taking action \n",
        "# action==1 ; UP\n",
        "# action==2 ; Down\n",
        "# action==3 ; Left\n",
        "# action==4 ; Rigth\n",
        "def find_next_block_CoOrdinate(action,i,j) :\n",
        "  if action == 1 :\n",
        "    return (i-1,j) \n",
        "  elif action == 2 :\n",
        "    return (i+1,j) \n",
        "  elif action == 3 :\n",
        "    return (i,j-1) \n",
        "  else :\n",
        "    return (i,j+1) \n"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erAeMiQdh1YF"
      },
      "source": [
        "# It check if after taking ACTION \"action\" in BLOCK (i,j)[\"either state\"] . wheather the new position is in Grid or NOT\n",
        "def Is_IN_Grid_After_taking_action(action,i,j) :\n",
        "  new_i,new_j = find_next_block_CoOrdinate(action,i,j) \n",
        "  if new_i<0 or new_i>3 or new_j<0 or new_j>3 : \n",
        "    return False \n",
        "  else :\n",
        "    return True "
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqfLXUorgGA6"
      },
      "source": [
        "# INPUT   :  matrix(\"value\"),ACTION(\"action\") , Current Block (i,j)[\" either state \"]\n",
        "# OUTPUT  :  updated state-value \n",
        "def update(value,action,i,j) :\n",
        "  sum = 0\n",
        "  new_i,new_j = find_next_block_CoOrdinate(action,i,j) \n",
        "  if new_i==0 and new_j==0 :           # checking if we entered top-left terminating state\n",
        "    sum = 0 \n",
        "  elif new_i==4 and new_j==4 :         # checking if we entered bottom-rigth terminating state\n",
        "    sum = 0\n",
        "  else :\n",
        "    sum  = sum + (-1)\n",
        "  if (j==3) :\n",
        "    #print(\"cordinage \",i,j)\n",
        "    #print(\"deb action \",action)\n",
        "    #print(\"debbug sum1 \",sum)\n",
        "    do = \"debugg\"\n",
        "  Is_IN_Grid = Is_IN_Grid_After_taking_action(action,i,j) \n",
        "  if Is_IN_Grid == True :                   # checking if are in Grid\n",
        "    sum =sum +  gamma*value[new_i][new_j]\n",
        "    if j==3 :\n",
        "      #print(\"db In Grid\")\n",
        "      do = \"debug\"\n",
        "  else :\n",
        "    sum =sum + gamma*value[i][j]\n",
        "    if j==3 :\n",
        "      #print(\"db OUT side grid\")\n",
        "      do = \"debug\"\n",
        "  if j==3 :\n",
        "    #print(\"db final sum \",sum)\n",
        "    do = \"debug\"\n",
        "  return sum "
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZUA-88wzC1V"
      },
      "source": [
        "def Max_Diff(prev,cur) :\n",
        "  list_difference = []\n",
        "  for i in range(4) :\n",
        "    for j in range(4) :\n",
        "      if not (  (i==0 and j==0) or (i==3 and j==3)  ) :\n",
        "        list_difference.append( abs( prev[i][j] - cur[i][j] ) )\n",
        "  return max(list_difference)"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2t0StdU5UfCy"
      },
      "source": [
        "#INPUT  : STATE-VALUE(\"value\") , PI(ACTION={1,2,3,4}|STATE={1,...14}) \n",
        "#OUTPUT : Evaluated \"STATE-VALUE\" ; Terminated when ration is less than 0.9 , means relativly Not much improvement ; formulae of ration is IN LAST IF CONDITION\n",
        "def policy_evaluation(value,action) :\n",
        "  print(\"---------Policy Evaluation\")\n",
        "  prev_value = copy.deepcopy(value)\n",
        "  previous_Max_loss= None \n",
        "  loop = 0\n",
        "  while(True) :\n",
        "    #print(\"LOOP \",loop+1)\n",
        "    loop=loop+1\n",
        "    for i in range(4) :\n",
        "      for j in range(4) :\n",
        "        if not (  ( i==0 and j==0 ) or ( i==3 and j==3 ) )  :\n",
        "          value[i][j] = update(value,action[i][j],i,j)\n",
        "          if(j==3) :\n",
        "            #print(\"debbuging\\t\",i,j,value[i][j])\n",
        "            do = \"debug\"\n",
        "        #print(i,j,value[i][j]) \n",
        "    current_Max_loss = Max_Diff(prev_value,value) \n",
        "    if previous_Max_loss == None :\n",
        "      previous_Max_loss = current_Max_loss \n",
        "    else :\n",
        "      if abs(current_Max_loss-previous_Max_loss) < 1.2 :\n",
        "        break \n",
        "\n",
        "  "
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLk8ZMEFlPol"
      },
      "source": [
        "# INPUT : list of rewards and corresponding actions \n",
        "#OUPUT  : Maximum reward from the list , action for maximum reward\n",
        "def Find_optimal_action_except_current_action(new_reward,new_action) :\n",
        "  maximum_reward = max(new_reward)     # Getting maximum reward\n",
        "  actions = []                         # List for storing action's taken for maximum rewards. Since, multiple action can give same reward(And that same is maximum TOO)\n",
        "  for i in range( len(new_reward) ) :\n",
        "    if maximum_reward == new_reward[i] :\n",
        "      actions.append( new_action[i] )\n",
        "  return random.choice(actions),maximum_reward"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1puml1iYqCz"
      },
      "source": [
        "#INPUT  :  STATE-VALUE , ACTION \n",
        "#OUTPUT :  OLD STATE-ACTION MATRIX(save old state-action in value in new matrix , before improvement  )  , Improves old state-action matrix\n",
        "def policy_improvement(value,actions) :\n",
        "  print(\"--------------Doing Policy Improvement\")\n",
        "  print(\"----OLD state action\")\n",
        "  print(actions)\n",
        "  old_state_action = copy.deepcopy(actions) \n",
        "  for i in range(4) :\n",
        "    for j in range(4) :\n",
        "      if not ( ( i==0 and j==0 ) or (i==4 or j==4) )  :               # Discarding terminating states for improvement\n",
        "        current_optimal_action = actions[i][j]\n",
        "        current_optimal_reward = update( value,action[i][j],i,j )\n",
        "        new_reward_list = [] \n",
        "        new_action_list = []\n",
        "        for action_code in range(1,5) :                                 # Looping over all action {1,2,3,4}\n",
        "          if not ( action_code == current_optimal_action ) :          # Discarding current state-action and only taking other actions\n",
        "            new_reward_list.append( update(value,action_code,i,j) )\n",
        "            new_action_list.append( action_code ) \n",
        "        new_action,new_reward = Find_optimal_action_except_current_action( new_reward_list , new_action_list ) \n",
        "        if ( new_reward > current_optimal_reward ) :\n",
        "          actions[i][j] = new_action \n",
        "        else :\n",
        "            Do = \"DO NOTHING : Current state-action is optimal\" \n",
        "  return old_state_action \n",
        "        "
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81oCH8lawUtl"
      },
      "source": [
        ""
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PO9KJlaBv8lS"
      },
      "source": [
        "def Is_policy_stable(old,cur) :\n",
        "  answer = True \n",
        "  for i in range(4) :\n",
        "    for j in range(4) :\n",
        "      if not( old[i][j] == cur[i][j] ) :\n",
        "        answer = False \n",
        "        break \n",
        "  return answer "
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqjofZSPT490"
      },
      "source": [
        "def policy_iteration(value,number_iter,actions) :\n",
        "  for i in range(number_iter) :\n",
        "    print(\"\\n\\n\\nPOLICY ITERATION NUMBER\\t\",i+1)\n",
        "    policy_evaluation(value,actions) \n",
        "    print(\"STATE-VALUE MATRIX\")\n",
        "    print(value)\n",
        "    old_policy = policy_improvement(value,actions) \n",
        "    print(\"----UPDATED POLICY\")\n",
        "    print(action) \n",
        "    if ( Is_policy_stable(old_policy,actions) == True ) :\n",
        "     print(\"\\npolicy stabalize at Iteration \",i+1)\n",
        "     break \n",
        "  "
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_2vwkNt-pQf"
      },
      "source": [
        "Below max_policy_iteration is hard coded can be changed.\n",
        "\n",
        "max_policy_iteration is the number of Maximum Iterations.\n",
        "\n",
        "```\n",
        "`# This is formatted as code`\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZBRhU_r_bGP"
      },
      "source": [
        "max_policy_iteration  = 10 "
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fP6WPFo3yKrP",
        "outputId": "bd87fc12-3669-4496-9e58-42f003a302ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "policy_iteration(value,max_policy_iteration,action)"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "POLICY ITERATION NUMBER\t 1\n",
            "---------Policy Evaluation\n",
            "STATE-VALUE MATRIX\n",
            "[[  0.     0.    -2.71  -2.71]\n",
            " [-10.    -1.    -1.9   -1.9 ]\n",
            " [-10.   -10.   -10.    -1.  ]\n",
            " [-10.   -10.   -10.     0.  ]]\n",
            "--------------Doing Policy Improvement\n",
            "----OLD state action\n",
            "[[-1  3  2  2]\n",
            " [ 2  1  3  2]\n",
            " [ 1  3  3  2]\n",
            " [ 3  2  3 -1]]\n",
            "----UPDATED POLICY\n",
            "[[-1  3  3  2]\n",
            " [ 1  1  3  2]\n",
            " [ 1  1  4  2]\n",
            " [ 3  2  4 -1]]\n",
            "\n",
            "\n",
            "\n",
            "POLICY ITERATION NUMBER\t 2\n",
            "---------Policy Evaluation\n",
            "STATE-VALUE MATRIX\n",
            "[[  0.     0.    -1.    -2.71]\n",
            " [  0.    -1.    -1.9   -1.9 ]\n",
            " [ -1.    -1.9   -1.9   -1.  ]\n",
            " [-10.   -10.    -1.     0.  ]]\n",
            "--------------Doing Policy Improvement\n",
            "----OLD state action\n",
            "[[-1  3  3  2]\n",
            " [ 1  1  3  2]\n",
            " [ 1  1  4  2]\n",
            " [ 3  2  4 -1]]\n",
            "----UPDATED POLICY\n",
            "[[-1  3  3  3]\n",
            " [ 1  1  3  2]\n",
            " [ 1  1  4  2]\n",
            " [ 1  4  4 -1]]\n",
            "\n",
            "\n",
            "\n",
            "POLICY ITERATION NUMBER\t 3\n",
            "---------Policy Evaluation\n",
            "STATE-VALUE MATRIX\n",
            "[[ 0.   0.  -1.  -1.9]\n",
            " [ 0.  -1.  -1.9 -1.9]\n",
            " [-1.  -1.9 -1.9 -1. ]\n",
            " [-1.9 -1.9 -1.   0. ]]\n",
            "--------------Doing Policy Improvement\n",
            "----OLD state action\n",
            "[[-1  3  3  3]\n",
            " [ 1  1  3  2]\n",
            " [ 1  1  4  2]\n",
            " [ 1  4  4 -1]]\n",
            "----UPDATED POLICY\n",
            "[[-1  3  3  3]\n",
            " [ 1  1  3  2]\n",
            " [ 1  1  4  2]\n",
            " [ 1  4  4 -1]]\n",
            "policy stabalize at Iteration  2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TU3ZAGST_vuc",
        "outputId": "80727978-1f53-4787-db0b-43d6f8d4e9ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print(\"Final STATE VALUE\")\n",
        "print(value)"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final STATE VALUE\n",
            "[[ 0.   0.  -1.  -1.9]\n",
            " [ 0.  -1.  -1.9 -1.9]\n",
            " [-1.  -1.9 -1.9 -1. ]\n",
            " [-1.9 -1.9 -1.   0. ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74rE8xAJ_26N",
        "outputId": "df32d682-9d72-42c6-817b-31e0239eca6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# 1== UP , 2==DOWN , 3==LEFT , 4 ==RIGHT\n",
        "print(\"FINAL STATE ACTION\")\n",
        "print(action)"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FINAL STATE ACTION\n",
            "[[-1  3  3  3]\n",
            " [ 1  1  3  2]\n",
            " [ 1  1  4  2]\n",
            " [ 1  4  4 -1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAjoT6Yz-d4Z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}